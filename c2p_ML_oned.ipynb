{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "839f5ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Physics modules\n",
    "from metric import metric\n",
    "from hybrid_eos import hybrid_eos\n",
    "\n",
    "# Numpy and matplotlib\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init \n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bb03abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fae2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class c2p_NN(nn.Module):\n",
    "    \"\"\"\n",
    "    This class defines a neural network model for the conservative-to-primitive variable transformation.\n",
    "    The network consists of several fully connected layers with ReLU activation functions and batch normalization.\n",
    "    The input to the network is a tensor of conservative variables, and the output is a tensor of primitive variables.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_layers=3, neurons=50):\n",
    "        super(c2p_NN, self).__init__()\n",
    "        self.activation = nn.Tanh \n",
    "        input_dim = 3 \n",
    "        output_dim = 1 \n",
    "        layers = []\n",
    "        layers.append(nn.BatchNorm1d(input_dim))\n",
    "        layers.append(nn.Linear(input_dim, neurons))  # Input layer\n",
    "        layers.append(self.activation())  # Activation\n",
    "        for _ in range(hidden_layers):\n",
    "            layers.append(nn.Linear(neurons, neurons))  # Hidden layers\n",
    "            layers.append(nn.BatchNorm1d(neurons)) # Normalization\n",
    "            layers.append(self.activation())  # Activation\n",
    "            \n",
    "        layers.append(nn.Linear(neurons, output_dim))  # Output layer\n",
    "        # Last relu because z >= 0 \n",
    "        layers.append(nn.ReLU()                     )  # Activation\n",
    "        self.network = nn.Sequential(*layers)\n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        \"\"\" Initialize weights using Kaiming initialization \"\"\"\n",
    "        if isinstance(module, nn.Linear):\n",
    "            #init.kaiming_normal_(module.weight, nonlinearity=\"relu\")\n",
    "            init.xavier_normal(module.weight)\n",
    "            init.zeros_(module.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e07a526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minkowski metric \n",
    "eta = metric(\n",
    "torch.eye(3,device=device), torch.zeros(3,device=device), torch.ones(1,device=device)\n",
    ")\n",
    "# Gamma = 2 EOS with ideal gas thermal contrib \n",
    "eos = hybrid_eos(100,2,1.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4374bba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_initial_state_random(metric,eos,N,device,lrhomin=-12,lrhomax=-2.8,ltempmin=-1,ltempmax=2.3,Wmin=1,Wmax=2):\n",
    "    # Get W, rho and T \n",
    "    W = Wmin + (Wmax-Wmin) * torch.rand(N,device=device)\n",
    "    rho = 10**( lrhomin + (lrhomax-lrhomin) * torch.rand(N,device=device) )\n",
    "    T = 10**( ltempmin + (ltempmax-ltempmin) * torch.rand(N,device=device) )\n",
    "    # Call EOS to get press and eps \n",
    "    press,eps = eos.press_eps__temp_rho(T,rho)\n",
    "    # Compute z \n",
    "    Z = torch.sqrt(1 - 1/W**2) * W \n",
    "    \n",
    "    # Compute conserved vars \n",
    "    sqrtg = metric.sqrtg \n",
    "    u0 = W / sqrtg \n",
    "    dens = sqrtg * W * rho \n",
    "    \n",
    "    rho0_h = rho * ( 1 + eps ) + press \n",
    "    g4uptt = - 1/metric.alp**2 \n",
    "    Tuptt = rho0_h * u0**2 + press * g4uptt \n",
    "    tau = metric.alp**2 * sqrtg * Tuptt - dens \n",
    "    \n",
    "    S = torch.sqrt((W**2-1)) * rho0_h * W\n",
    "    # Assemble output \n",
    "    C = torch.cat((dens.view(-1,1)/metric.sqrtg,tau.view(-1,1)/dens.view(-1,1),S.view(-1,1)/dens.view(-1,1)),dim=1)\n",
    "    return C, Z.view(-1,1)\n",
    "\n",
    "def setup_initial_state_meshgrid(metric,eos,N,device,lrhomin=-12,lrhomax=-2.8,ltempmin=-1,ltempmax=2.3,Wmin=1,Wmax=2):\n",
    "    # Get W, rho and T \n",
    "    W = torch.linspace(Wmin,Wmax,N,device=device)\n",
    "    rho = 10**( torch.linspace(lrhomin,lrhomax,N,device=device) )\n",
    "    T = 10**( torch.linspace(ltempmin,ltempmax,N,device=device) )\n",
    "    W, rho, T = torch.meshgrid(W,rho,T, indexing='ij')\n",
    "    \n",
    "    W = W.flatten() \n",
    "    rho = rho.flatten()\n",
    "    T = T.flatten() \n",
    "    \n",
    "    # Call EOS to get press and eps \n",
    "    press,eps = eos.press_eps__temp_rho(T,rho)\n",
    "    # Compute z \n",
    "    Z = torch.sqrt(1 - 1/W**2) * W \n",
    "    \n",
    "    # Compute conserved vars \n",
    "    sqrtg = metric.sqrtg \n",
    "    u0 = W / sqrtg \n",
    "    dens = sqrtg * W * rho \n",
    "    \n",
    "    rho0_h = rho * ( 1 + eps ) + press \n",
    "    g4uptt = - 1/metric.alp**2 \n",
    "    Tuptt = rho0_h * u0**2 + press * g4uptt \n",
    "    tau = metric.alp**2 * sqrtg * Tuptt - dens \n",
    "    \n",
    "    S = torch.sqrt((W**2-1)) * rho0_h * W\n",
    "    # Assemble output \n",
    "    C = torch.cat((dens.view(-1,1)/metric.sqrtg,tau.view(-1,1)/dens.view(-1,1),S.view(-1,1)/dens.view(-1,1)),dim=1)\n",
    "    return C, Z.view(-1,1)\n",
    "    \n",
    "def sanity_check(Z,C, metric, eos):\n",
    "    t,q,r = torch.split(C,[1,1,1], dim=1)\n",
    "    htilde = h__z(Z,C,eos)\n",
    "    \n",
    "    return torch.mean((Z - r/htilde)**2)\n",
    "\n",
    "def W__z(z):\n",
    "    return torch.sqrt(1 + z**2)\n",
    "\n",
    "def rho__z(z,C):\n",
    "    return C[:,0].view(-1,1) / W__z(z)\n",
    "\n",
    "def eps__z(z,C):\n",
    "    q = C[:,1].view(-1,1)\n",
    "    r = C[:,2].view(-1,1)\n",
    "    W = W__z(z)\n",
    "    return W * q - z * r + z**2/(1+W)\n",
    "\n",
    "def a__z(z,C,eos):\n",
    "    eps = eps__z(z,C)\n",
    "    rho = rho__z(z,C)\n",
    "    press = eos.press__eps_rho(eps,rho)\n",
    "    return press/(rho*(1+eps))\n",
    "\n",
    "def h__z(z,C,eos):\n",
    "    eps = eps__z(z,C)\n",
    "    a = a__z(z,C,eos)\n",
    "    return (1 + eps)*(1+a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e97a77e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.8528e-13, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "C, Z = setup_initial_state_meshgrid(eta,eos,20,device)\n",
    "err = sanity_check(Z,C,eta,eos)\n",
    "\n",
    "print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db2433a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(model, C, eos, metric):\n",
    "    '''\n",
    "    Eq (C3) of https://arxiv.org/pdf/1306.4953.pdf\n",
    "    '''\n",
    "    Z_pred = model(C)\n",
    "    htilde = h__z(Z_pred,C,eos)\n",
    "    return F.mse_loss(Z_pred,C[:,2].view(-1,1)/htilde)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "413cdbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_c2p_model(model,optimizer,scheduler,num_epochs,C,eos,metric):\n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss = compute_loss(model,C,eos,metric)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "            scheduler.step(loss)\n",
    "        else:\n",
    "            scheduler.step()\n",
    "        \n",
    "        # Print progress\n",
    "        if epoch % 1000 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.item():.6f}, LR: {optimizer.param_groups[0]['lr']:.6e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c3d2413",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/rafast/musolino/pyenv/numrel/lib/python3.10/site-packages/torch/nn/modules/linear.py:125: UserWarning: Attempting to use hipBLASLt on an unsupported architecture! Overriding blas backend to hipblas (Triggered internally at ../aten/src/ATen/Context.cpp:296.)\n",
      "  return F.linear(input, self.weight, self.bias)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.168570, LR: 1.000000e-03\n",
      "Epoch 1000, Loss: 0.002842, LR: 1.000000e-03\n",
      "Epoch 2000, Loss: 0.002714, LR: 1.000000e-03\n",
      "Epoch 3000, Loss: 0.002533, LR: 1.000000e-03\n",
      "Epoch 4000, Loss: 0.002506, LR: 1.000000e-03\n",
      "Epoch 5000, Loss: 0.002234, LR: 1.000000e-03\n",
      "Epoch 6000, Loss: 0.002238, LR: 1.000000e-03\n",
      "Epoch 7000, Loss: 0.002077, LR: 1.000000e-03\n",
      "Epoch 8000, Loss: 0.001993, LR: 1.000000e-03\n",
      "Epoch 9000, Loss: 0.001962, LR: 5.000000e-04\n",
      "Final loss: 0.0003262293175794184\n"
     ]
    }
   ],
   "source": [
    "neurons = 50 \n",
    "layers  = 4 \n",
    "\n",
    "net = c2p_NN(hidden_layers=layers,neurons=neurons).to(device)\n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=500)\n",
    "\n",
    "train_c2p_model(net,optimizer,scheduler,10000,C,eos,eta)\n",
    "\n",
    "# Do one more step with LBFGS\n",
    "optimizer = torch.optim.LBFGS(net.parameters(), lr=1.0, max_iter=500)\n",
    "\n",
    "def closure():\n",
    "    optimizer.zero_grad()\n",
    "    loss = compute_loss(net, C, eos, eta)\n",
    "    loss.backward()\n",
    "    return loss\n",
    "\n",
    "optimizer.step(closure)\n",
    "\n",
    "loss_value = closure()  # Compute the loss\n",
    "print(f\"Final loss: {loss_value.item()}\")  \n",
    "\n",
    "torch.save(net.state_dict(), f\"model_L{layers}N{neurons}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cba15dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity check:  tensor(2.1997e-13)\n",
      "Network error:  tensor(0.0071, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "C_test, Z_test = setup_initial_state_random(eta,eos,1000)\n",
    "Z_pred = net(C_test)\n",
    "print(\"Sanity check: \",sanity_check(Z_test,C_test,eta,eos))\n",
    "print(\"Network error: \", F.mse_loss(Z_pred,Z_test) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d908425a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.linspace(1,2,100)\n",
    "y = torch.linspace(1,2,100)\n",
    "z = torch.linspace(1,2,100)\n",
    "X,Y,Z = torch.meshgrid(x,y,z)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "numrel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
